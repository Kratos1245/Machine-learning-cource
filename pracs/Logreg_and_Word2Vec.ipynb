{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "matplotlib.rcParams['figure.figsize'] = (8, 8)\n",
    "sns.set_style('whitegrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1.1 [6 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте свой класс LogReg - собственная имплементация логистической регрессии с l2 регуляризацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogReg(BaseEstimator):\n",
    "    def __init__(self, lambda_2=1.0, gd_type='full', batch_size=None,\n",
    "                 tolerance=1e-4, max_iter=1000, w0=None, alpha=1e-3, \n",
    "                 verbose=False):\n",
    "        \"\"\"\n",
    "        lambda_2: L2 regularization param value\n",
    "        gd_type: 'full' or 'stochastic'\n",
    "        tolerance: for stopping gradient descent\n",
    "        max_iter: maximum number of steps in gradient descent\n",
    "        w0: np.array of shape (d) - init weights\n",
    "        alpha: learning rate\n",
    "        \"\"\"\n",
    "        self.lambda_2 = lambda_2\n",
    "        self.gd_type = gd_type\n",
    "        self.tolerance = tolerance\n",
    "        self.max_iter = max_iter\n",
    "        self.batch_size = None if self.gd_type == 'full' else batch_size\n",
    "        if self.batch_size is None and self.gd_type != 'full':\n",
    "            raise Exception('Need to specify batch size!')\n",
    "        self.w0 = w0\n",
    "        self.alpha = alpha\n",
    "        self.w = None\n",
    "        self.loss_history = []\n",
    "        self.w_history = []\n",
    "        self.grad_norm_history = []\n",
    "        self.verbose = verbose\n",
    "        # Могут быть ешё параметры\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"\n",
    "        Просто вычисляет сигмоиду\n",
    "        \"\"\"\n",
    "        # Ваш код здесь:\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (N, d)\n",
    "        y: np.array of shape (N, 1)\n",
    "        \"\"\"\n",
    "      # Ваш код здесь:\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (N, d)\n",
    "        ---\n",
    "        output: np.array of shape (N, 2) where\n",
    "        first column has probabilities of -1\n",
    "        second column has probabilities of +1\n",
    "        \"\"\"\n",
    "        # Ваш код здесь:\n",
    "        \n",
    "    \n",
    "    def calc_gradient(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (N, d) (N is equal to batch_size if gd_type=\"stochastic\")\n",
    "        y: np.array of shape (N, 1)\n",
    "        ---\n",
    "        output: np.array of shape (d, 1)\n",
    "        \"\"\"\n",
    "        # Ваш код здесь:\n",
    "        \n",
    "\n",
    "    def calc_loss(self, X, y):\n",
    "        \"\"\"\n",
    "        X: np.array of shape (N, d)\n",
    "        y: np.array of shape (N, 1)\n",
    "        ---\n",
    "        output: float \n",
    "        \"\"\" \n",
    "        # Ваш код здесь:\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сгенерируем модельную выборку, для проверки нашего класса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 100000\n",
    "X = np.random.normal(loc=0, scale=4, size=(N))\n",
    "X = np.hstack((\n",
    "    np.ones(X.shape[0]).reshape(-1, 1),\n",
    "    X.reshape(-1, 1)\n",
    "))\n",
    "y = (\n",
    "    2*(3*X[:, 1] + np.random.normal(scale=4, size=N) > 0) - 1\n",
    ").astype(int).reshape(-1, 1)\n",
    "w_init = np.random.randint(-10, 10, size=(2, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разделите данные на обучение и контроль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 1.2 [3 балла]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите Вашу реализацию логрега на обучающей выборке и протестируйте качество на котрольной выборке. Нарисуйте графики лосса в зависимости от числа итераций для обучения для полного градиентного спуска, а так же для стохастического градиентного спуска c различной величиной батча (батч размера 1, 10, 20, 30, половина выборки, вся выборка) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2\n",
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этой части задания Вам необходимо будет реализовать алгоритм поиска эмбеддингов для слов Word2Vec (модификация SkipGramNS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите лемматизированные тексты lemm_wiki_texts.txt для обучения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка текстов\n",
    "with open('../lemm_wiki_texts.txt', mode='r', encoding='utf-8') as f:\n",
    "    lemm_wiki_texts = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# у вас должно получиться 1095910 текстов - проверьте\n",
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.0 [1 балл]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте функцию, которая принимает на вход список текстов (такой например, как lemm_wiki_texts) и выдаёт список списков, где каждый внутренний список содержит 3 элемента: 2 слов и 1 число (0 или 1), символизирующее, находится ли пара слов в одном контексте, или нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_text_for_skipgramns(texts: list, context_width: int=5, negative_samples: int=10) -> list:\n",
    "    \"\"\"\n",
    "    texts: список текстов, по которому необходимо будет составить итоговый список\n",
    "    context_width: ширина окна. Слова, находящиеся в одном окне считаются из одного контекста\n",
    "    negative_samples: сколько негативных примеров генерировать на каждый положительный пример.\n",
    "    return: resulting_list: Пример: [\n",
    "    [\"мама\", \"готовит\", 1],\n",
    "    ['мама', \"океан\", 0],\n",
    "    [\"надёжный\", \"подрядчик\", 1],\n",
    "    [\"надёжный\", \"мороженое\", 0],\n",
    "    ...\n",
    "    ]\n",
    "    \"\"\"\n",
    "    # Ваш код здесь:\n",
    "    \n",
    "    return resulting_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.1 [15 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализуйте класс, который будет использоваться для обучения Word2Vec векторов. Реализовывать можно как на голом numpy, так и с помощью вспомогательных библиотек. Рекомендуется освоить библиотеку, [pytorch](https://pytorch.org/). Во-первых, она будет в дальнейшем использоваться, при прохождении нейросетей, а во-вторых, есть видео, которое шаг за шагом позволяет реализовать Word2Vec алгоритм на pytorch:\n",
    "https://stepik.org/lesson/247966/step/1?unit=220078 (P.S. Можно просто реализовать Word2Vec точно так же, как на видео)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "У этого класса обязательно должен быть метод, который \"обучает\" эмбеддинги. Результатом обучения должен быть словарь (dict), ключами которого являются слова, а значениями - эмбеддинги этих слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 2.2 [5 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создайте класс, для удобного обращения с обученными векторами"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings:\n",
    "    \n",
    "    def __init__(self, embedding_dim: int, word2vec_dict: dict):\n",
    "        \"\"\"\n",
    "        embedding_dim: размерность Word2Vec векторов\n",
    "        word2vec_dict: полученный словарь с эмбеддингами\n",
    "        \"\"\"\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.word2vec_dict = word2vec_dict\n",
    "        # Могут быть доп параметры\n",
    "        \n",
    "    def get_vector(self, word: str) -> np.array:\n",
    "        \"\"\"\n",
    "        word: слово, для которого мы хотим получить эмбеддинг \n",
    "        return: вектор слова\n",
    "        \"\"\"\n",
    "        # Ваш код здесь:\n",
    "        \n",
    "    def get_n_most_similar(self, word: str, top_n_similar: int) -> list:\n",
    "        \"\"\"\n",
    "        word: слово, для которого хотим получить похожие \n",
    "        top_n_similar: сколько похожих слов выдать\n",
    "        return: список таплов (tuple): слово и косинусное расстояние для слова word от текущего слова,\n",
    "        пример: word = 'спортсмен', return: [(\"спорт\", 0.3), ('чемпион', 0.33), (\"атлет\", 0.35), ...]\n",
    "        \"\"\"\n",
    "        # Ваш код здесь:\n",
    "        \n",
    "    def analogy(self, a1: str, a2: str, b1: str, top_n_similar: int) -> list:\n",
    "        \"\"\"\n",
    "        Осуществяет поиск решения уравнения: a1 - a2 + b1 = x\n",
    "        Пример: для a1='король', a2='мужчина', b1='женщина', ожидается увидеть в ответ x='королева'\n",
    "        return: список из таплов (tuple), как и в методе get_n_most_similar \n",
    "        \"\"\"\n",
    "        # Ваш код здесь:\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.3 [бонус] [3 балла]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Найдите несколько интересных аналогий в словах (что-то типо: король-мужчина+женщина=королева)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.4 [3 балла]\n",
    "#### Обучение из коробки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите Word2Vec \"из коробки\" с помощью библиотеки [gensim](https://radimrehurek.com/gensim/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.5 [5 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По смыслу ожидается, что слова относящиеся к одной тематике должны чаще встречаться в одном контексте, а значит и косинусное расстояние между их эмбеддингами должно быть меньше, чем расстояние между произвольной парой слов. Возьмите список из 500-1000 слов, относящихся к каким-нибудь 3-5 тематикам (придумайте сами каким: компьютеры, природа, история, и тд.), снизьте размерность этих слов (их эмбеддингов) с помощью TSNE и изобразите полученные объекты на плоскости. (Ожидается что слова из одной тематики будут образовывать кластеры, которые отдалены от кластеров, относящихся к другой тематике)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Классификация категории товара"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом задании Вам предстоит поучастовать в конкурсе от Авито: https://www.kaggle.com/c/texts-classification-ml-hse-2019/leaderboard\n",
    "\n",
    "Задача конкурса - по заголовку объявления и текстовому описанию классифицировать объявление."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузите train.csv и test.csv наборы данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = # Ваш код здесь\n",
    "test = # Ваш код здесь"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.1 [3 балла]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проведите лемматизацию текстов (для этого Вам может понадобиться пакет pymorphy2: https://pymorphy2.readthedocs.io/en/latest/user/guide.html#id3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.2 [3 балла]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Удалите \"стоп-слова\" (предлоги, междометия, ...). Поясните, как именно Вы выбирали стоп-слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.3 [6 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осуществите Tf-Idf преобразование текстов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разбейте данные на обучение и валидацию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tr_idx - индексы объектов из train, на которых будем обучать модель\n",
    "# val_idx - индексы объектов из train, на которых будем тестировать обученную модель\n",
    "np.random.seed(10) # не меняем seed!\n",
    "tr_idx, val_idx = np.split(\n",
    "    np.random.permutation(np.arange(len(train))), [int(np.floor(len(train)*0.7))]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучите логистическую регрессию на объектах с индексами tr_idx, которую Вы написали в Части 1. В качестве матрицы объект-признак возьмите TfIdf-матрицу, полученную вначале задания 3.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте качество полученной модели на отложенной части (на объектах с индексами val_idx). В качестве метрики возьмите ту, которая используется в конкурсе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь обучите логистическую регрессию из sklearn и сравните качество, с Вашей реализацией"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.4 [6 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь попробуйте обучить логистическую регрессию (из sklearn), но где в качестве признакового описания текста возьмите не матрицу TfIfd, а средневзвешенное Word2Vec описание текста, то есть, для каждого слова из текста вычисляем его Word2Vec эмбеддинг и суммируем эти вектора с некоторыми весами. В качестве весом можно взять значения idf значение слова. Если какого-то слова нет в вашем словаре Word2Vec, то его можно заменить нулевым вектором, или вектором средних из всех Word2Vec векторов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посчитайте качество данной модели на отложенной выборке (выборке с индексами val_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Задание 3.5 [бонус] [10 баллов]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Постарайтесь получить качество на лидерборде выше, чем у baseline решения (0.84726). В данном пункте Вы не ограничены ни моделями, которые можно применять, ни признаками, и даже можно каким-то образом обогащать данные. (Например: можно распрарсить Авито, найти объявления, которые в тесте и взять оттуда истинный класс)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ваш код здесь:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
